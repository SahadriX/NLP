# -*- coding: utf-8 -*-
"""NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iTTqxgcPH8XKFpuoAyMTzRsS48N-tfHq
"""

import nltk#nltk is the library used for nlp

# Download the 'punkt_tab' data package
nltk.download('punkt_tab')

data = "Hi my name is Sahadri."
tokens = nltk.sent_tokenize(data)#line tokenization
print(tokens)

tokens = nltk.word_tokenize(data)#word tokenization
print(tokens)

type(tokens)

len(tokens)

from nltk.stem import PorterStemmer
ps= PorterStemmer()

from nltk.stem import PorterStemmer
ps= PorterStemmer()

words=["liked","liking","likes"]
for i in words :
  print(i,":",ps.stem(i))

import nltk
nltk.download("wordnet")
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
words=["liked","liking","likes"]
for i in words :
  print(i,":",lemmatizer.lemmatize(i))

import spacy

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Input sentence
doc = nlp("The geese are flying in the sky.")

# Lemmatization
for token in doc:
    print(f"{token.text} â†’ {token.lemma_}")

from nltk.corpus import stopwords
nltk.download('stopwords')
data="messi is the best player in the world"
stop_words = set(stopwords.words('english'))
print(stopwords.words()[620:680])

import nltk
nltk.download("punkt")
data=nltk.word_tokenize(data)
print(data)

import numpy as np
import pandas as pd

import numpy as np
import pandas as pd

# Try to handle the error by telling pandas to ignore bad lines
df = pd.read_csv("/content/IMDB_Dataset_Cleaned.csv", on_bad_lines='skip',quoting=3)
# If the above doesn't work, you can try specifying the quoting behavior
# df = pd.read_csv("/content/IMDB_Dataset_Cleaned.csv", quoting=pd.QUOTE_NONE, on_bad_lines='skip')

df.head()

df.to_csv("IMDB_Dataset_Fixed.csv", index=False, encoding="utf-8")
from google.colab import files
files.download("IMDB_Dataset_Fixed.csv")

df=pd.read_csv("/content/IMDB_Dataset_Fixed.csv")
df.head()

df = df.dropna()  # Removes all NaN rows
df.reset_index(drop=True, inplace=True)

df[df['review'].isna()]

df.to_csv("IMDB_Dataset_Cleaned.csv", index=False, encoding="utf-8")

df.head()

df.columns

df.describe()

import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelBinarizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud, STOPWORDS
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize

import spacy
import re, string, unicodedata
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from textblob import TextBlob
from textblob import Word
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from bs4 import BeautifulSoup

nltk.download('stopwords')

from nltk.tokenize.toktok import ToktokTokenizer
import nltk

tokenizers = ToktokTokenizer() # Corrected instantiation
stopword_list = nltk.corpus.stopwords.words('english')

def noise_removal_text(text):
    # Check if the input is a string
    if isinstance(text, str):
        soup = BeautifulSoup(text, "html.parser")
        text = soup.get_text()
        text = re.sub('\[[^]]*\]', '', text)
    # If it's not a string (e.g., float/NaN), return it as is or handle it differently
    else:
        # Choose one of the options below:
        # 1. Return the original value as is
        return text
        # 2. Return an empty string
        # return ""
        # 3. Raise a more informative error
        # raise ValueError(f"Expected a string, but got {type(text)}")

    return text

df["review"]=df["review"].apply(noise_removal_text)

def noise_removal_text(text):
    # Check if the input is a string
    if isinstance(text, str):
        soup = BeautifulSoup(text, "html.parser")
        text = soup.get_text()
        text = re.sub('\[[^]]*\]', '', text)
    # If it's not a string (e.g., float/NaN), return it as is or handle it differently
    else:
        # Choose one of the options below:
        # 1. Return the original value as is
        return text
        # 2. Return an empty string
        # return ""
        # 3. Raise a more informative error
        # raise ValueError(f"Expected a string, but got {type(text)}")

    return text

df.head()

def stemmer(text):
    if not isinstance(text, str):  # Handle NaN or non-string values
        return ''
    return ' '.join([ps.stem(word) for word in text.split()])
    df["review"] = df["review"].fillna('').astype(str).apply(stemmer)

df.head()

def removing_stopwords (text, is_lower_case=False): #Tokenization of text
  tokenizers=ToktokTokenizer()
#Setting English stopwords
  tokens =tokenizers.tokenize (text)
  tokens = [i.strip() for i in tokens]
  if is_lower_case:
    filtokens = [i for i in tokens if token not in stopword_list]
  else:
    filtokens = [i for i in tokens if i.lower() not in stopword_list]
    filtered_texts = ' '.join(filtokens)
  return filtered_texts

df['review'] = df['review'].apply(removing_stopwords)

df.head()

from sklearn.model_selection import train_test_split
test_size = 0.2  # 20% test data
train_reviews_df, test_reviews_df = train_test_split(df.review, test_size=test_size, random_state=42)

cv = CountVectorizer(min_df=1, max_df=1.0, binary=False, ngram_range=(1, 3))  # Change min_df to 1
cv_train = cv.fit_transform(train_reviews_df)
cv_test = cv.transform(test_reviews_df)
print('BOW_cv_train: ', cv_train.shape)
print('BOW_cv_test: ', cv_test.shape)

df['sentiment'] = df['sentiment'].fillna('').astype(str)
label=LabelBinarizer()
sentiment_df=label.fit_transform(df['sentiment'])
print(sentiment_df.shape)

#Tfidf vectorizer
tf=TfidfVectorizer(min_df=1,max_df=1.0, use_idf=True, ngram_range=(1,3)) #transformed train reviews # Changed min_df to 1
tf_train=tf.fit_transform(train_reviews_df)
#transformed test reviews
tf_test=tf.transform(test_reviews_df)
print('Tfidf_train: ',tf_train.shape)
print('Tfidf_test: ',tf_test.shape)

label=LabelBinarizer()
sentiment_df=label.fit_transform(df['sentiment'])
print(sentiment_df.shape)

train_df=df.sentiment[:3000]

test_df=df.sentiment[3000:]

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Initialize Logistic Regression Model
logistic = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)

# 1. Train the Logistic Regression Model using Bag-of-Words (BoW)
# Ensure that the number of labels matches the number of training samples
# Train the Logistic Regression Model using Bag-of-Words (BoW)
lr_bow = logistic.fit(cv_train, sentiment_df[:len(train_reviews_df)])  # Ensure the labels match the training samples
print("BoW Model Trained Successfully")

# Train the Logistic Regression Model using TF-IDF
lr_tfidf = logistic.fit(tf_train, sentiment_df[:len(train_reviews_df)])  # Ensure the labels match the training samples
print("TF-IDF Model Trained Successfully")

# Evaluate both models using the test set

# Predict on test data using BoW model
y_pred_bow = lr_bow.predict(cv_test)
print("BoW Model Evaluation")
print("Accuracy: ", accuracy_score(sentiment_df[len(train_reviews_df):], y_pred_bow))
print(classification_report(sentiment_df[len(train_reviews_df):], y_pred_bow))
print(confusion_matrix(sentiment_df[len(train_reviews_df):], y_pred_bow))

# Predict on test data using TF-IDF model
y_pred_tfidf = lr_tfidf.predict(tf_test)
print("TF-IDF Model Evaluation")
print("Accuracy: ", accuracy_score(sentiment_df[len(train_reviews_df):], y_pred_tfidf))
print(classification_report(sentiment_df[len(train_reviews_df):], y_pred_tfidf))
print(confusion_matrix(sentiment_df[len(train_reviews_df):], y_pred_tfidf))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Ensure the number of samples in cv_train matches the number of labels in sentiment_df
# The correct slicing should match the length of the train data
train_labels = sentiment_df[:len(train_reviews_df)]

# 1. Train the Logistic Regression Model using Bag-of-Words (BoW)
lr_bow = logistic.fit(cv_train, train_labels)  # Use train labels corresponding to train reviews
print("BoW Model Trained Successfully")

# 2. Train the Logistic Regression Model using TF-IDF
lr_tfidf = logistic.fit(tf_train, train_labels)  # Use train labels corresponding to train reviews
print("TF-IDF Model Trained Successfully")

# Evaluate both models using the test set

# 3. Predict on test data using BoW model
y_pred_bow = lr_bow.predict(cv_test)
print("BoW Model Evaluation")
print("Accuracy: ", accuracy_score(sentiment_df[len(train_reviews_df):], y_pred_bow))  # Test labels should be sliced properly
print(classification_report(sentiment_df[len(train_reviews_df):], y_pred_bow))
print(confusion_matrix(sentiment_df[len(train_reviews_df):], y_pred_bow))

# 4. Predict on test data using TF-IDF model
y_pred_tfidf = lr_tfidf.predict(tf_test)
print("TF-IDF Model Evaluation")
print("Accuracy: ", accuracy_score(sentiment_df[len(train_reviews_df):], y_pred_tfidf))  # Test labels should be sliced properly
print(classification_report(sentiment_df[len(train_reviews_df):], y_pred_tfidf))
print(confusion_matrix(sentiment_df[len(train_reviews_df):], y_pred_tfidf))

# Ensure correct splitting of reviews and sentiments
train_reviews_df, test_reviews_df = train_test_split(df['review'], test_size=0.2, random_state=42)
train_labels = df['sentiment'][:len(train_reviews_df)]  # Ensure that the number of labels matches the number of train samples
test_labels = df['sentiment'][len(train_reviews_df):]  # Test labels should be from the remaining samples

# Check the number of samples in train_labels and cv_train
print("Train labels length: ", len(train_labels))
print("cv_train shape: ", cv_train.shape)

# Fit the model for Bag-of-Words (BoW)
lr_bow = logistic.fit(cv_train, train_labels)
print("BoW Model Trained Successfully")

# Fit the model for TF-IDF
lr_tfidf = logistic.fit(tf_train, train_labels)
print("TF-IDF Model Trained Successfully")

# Import necessary libraries
import numpy as np
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize.toktok import ToktokTokenizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import re
from bs4 import BeautifulSoup

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load your dataset
# Assuming you have a CSV with 'review' and 'sentiment' columns (sentiment: positive/negative)
df = pd.read_csv('# Import necessary libraries
import numpy as np
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize.toktok import ToktokTokenizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import re
from bs4 import BeautifulSoup

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load your dataset
# Assuming you have a CSV with 'review' and 'sentiment' columns (sentiment: positive/negative)
df = pd.read_csv('IMDB_Dataset_Fixed.csv')

# Display the first few rows of the dataframe
print(df.head())

# Preprocessing Steps:

# 1. Text cleaning (Remove HTML tags, special characters, etc.)
def clean_text(text):
    # Remove HTML tags
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text()

    # Remove non-alphabetic characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Convert to lowercase
    text = text.lower()

    return text

df['cleaned_review'] = df['review'].apply(clean_text)

# 2. Tokenization (Convert text into a list of tokens/words)
tokenizer = ToktokTokenizer()
df['tokens'] = df['cleaned_review'].apply(tokenizer.tokenize)

# 3. Removing stopwords
stopword_list = stopwords.words('english')
def remove_stopwords(tokens):
    return [word for word in tokens if word not in stopword_list]

df['tokens'] = df['tokens'].apply(remove_stopwords)

# 4. Stemming (Using Porter Stemmer)
ps = PorterStemmer()
def stem_text(tokens):
    return [ps.stem(word) for word in tokens]

df['tokens_stemmed'] = df['tokens'].apply(stem_text)

# 5. Lemmatization (Using WordNet Lemmatizer)
lemmatizer = WordNetLemmatizer()
def lemmatize_text(tokens):
    return [lemmatizer.lemmatize(word) for word in tokens]

df['tokens_lemmatized'] = df['tokens'].apply(lemmatize_text)

# 6. Join tokens back to string for model input
df['processed_review'] = df['tokens_lemmatized'].apply(lambda x: ' '.join(x))

# 7. Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['processed_review'], df['sentiment'], test_size=0.2, random_state=42)

# 8. Vectorization (BoW or TF-IDF)
# Using TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, ngram_range=(1, 2))
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# 9. Logistic Regression Model
logistic_model = LogisticRegression()
logistic_model.fit(X_train_tfidf, y_train)

# 10. Make predictions
y_pred = logistic_model.predict(X_test_tfidf)

# 11. Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print('Classification Report:\n', classification_report(y_test, y_pred))
print('Confusion Matrix:\n', confusion_matrix(y_test, y_pred))

# Example usage (prediction on new review)
new_review = "The movie was fantastic and very enjoyable"
processed_new_review = clean_text(new_review)
tokens_new_review = tokenizer.tokenize(processed_new_review)
tokens_no_stopwords = remove_stopwords(tokens_new_review)
tokens_stemmed = stem_text(tokens_no_stopwords)
tokens_lemmatized = lemmatize_text(tokens_stemmed)
processed_new_review = ' '.join(tokens_lemmatized)

# Vectorizing the new review using the trained TF-IDF vectorizer
new_review_vectorized = tfidf_vectorizer.transform([processed_new_review])

# Predict the sentiment of the new review
new_prediction = logistic_model.predict(new_review_vectorized)
print(f"Sentiment of the new review: {new_prediction[0]}")
')

# Display the first few rows of the dataframe
print(df.head())

# Preprocessing Steps:

# 1. Text cleaning (Remove HTML tags, special characters, etc.)
def clean_text(text):
    # Remove HTML tags
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text()

    # Remove non-alphabetic characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Convert to lowercase
    text = text.lower()

    return text

df['cleaned_review'] = df['review'].apply(clean_text)

# 2. Tokenization (Convert text into a list of tokens/words)
tokenizer = ToktokTokenizer()
df['tokens'] = df['cleaned_review'].apply(tokenizer.tokenize)

# 3. Removing stopwords
stopword_list = stopwords.words('english')
def remove_stopwords(tokens):
    return [word for word in tokens if word not in stopword_list]

df['tokens'] = df['tokens'].apply(remove_stopwords)

# 4. Stemming (Using Porter Stemmer)
ps = PorterStemmer()
def stem_text(tokens):
    return [ps.stem(word) for word in tokens]

df['tokens_stemmed'] = df['tokens'].apply(stem_text)

# 5. Lemmatization (Using WordNet Lemmatizer)
lemmatizer = WordNetLemmatizer()
def lemmatize_text(tokens):
    return [lemmatizer.lemmatize(word) for word in tokens]

df['tokens_lemmatized'] = df['tokens'].apply(lemmatize_text)

# 6. Join tokens back to string for model input
df['processed_review'] = df['tokens_lemmatized'].apply(lambda x: ' '.join(x))

# 7. Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['processed_review'], df['sentiment'], test_size=0.2, random_state=42)

# 8. Vectorization (BoW or TF-IDF)
# Using TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, ngram_range=(1, 2))
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# 9. Logistic Regression Model
logistic_model = LogisticRegression()
logistic_model.fit(X_train_tfidf, y_train)

# 10. Make predictions
y_pred = logistic_model.predict(X_test_tfidf)

# 11. Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print('Classification Report:\n', classification_report(y_test, y_pred))
print('Confusion Matrix:\n', confusion_matrix(y_test, y_pred))

# Example usage (prediction on new review)
new_review = "The movie was fantastic and very enjoyable"
processed_new_review = clean_text(new_review)
tokens_new_review = tokenizer.tokenize(processed_new_review)
tokens_no_stopwords = remove_stopwords(tokens_new_review)
tokens_stemmed = stem_text(tokens_no_stopwords)
tokens_lemmatized = lemmatize_text(tokens_stemmed)
processed_new_review = ' '.join(tokens_lemmatized)

# Vectorizing the new review using the trained TF-IDF vectorizer
new_review_vectorized = tfidf_vectorizer.transform([processed_new_review])

# Predict the sentiment of the new review
new_prediction = logistic_model.predict(new_review_vectorized)
print(f"Sentiment of the new review: {new_prediction[0]}")

import numpy as np
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize.toktok import ToktokTokenizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import re
from bs4 import BeautifulSoup

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load your dataset (make sure your CSV file path is correct)
df = pd.read_csv('IMDB_Dataset_Fixed.csv')

# Display the first few rows of the dataframe
print(df.head())

# Preprocessing Steps:

# 1. Handle NaN values by replacing them with an empty string or dropping them
df = df.dropna(subset=['review'])  # Drop rows with missing reviews
# Alternatively, you could replace NaN with a placeholder text:
# df['review'] = df['review'].fillna("No review")

# 2. Text cleaning (Remove HTML tags, special characters, etc.)
def clean_text(text):
    # Handle NaN or non-string values
    if isinstance(text, str):
        # Remove HTML tags
        soup = BeautifulSoup(text, "html.parser")
        text = soup.get_text()

        # Remove non-alphabetic characters
        text = re.sub(r'[^a-zA-Z\s]', '', text)

        # Convert to lowercase
        text = text.lower()

        return text
    else:
        return ""  # Return an empty string if the input is not a string (e.g., NaN)

# Clean the reviews by applying the clean_text function
df['cleaned_review'] = df['review'].apply(clean_text)

# Check the cleaned data
print(df['cleaned_review'].head())

# 3. Tokenization (Convert text into a list of tokens/words)
tokenizer = ToktokTokenizer()
df['tokens'] = df['cleaned_review'].apply(tokenizer.tokenize)

# 4. Removing stopwords
stopword_list = stopwords.words('english')
def remove_stopwords(tokens):
    return [word for word in tokens if word not in stopword_list]

df['tokens'] = df['tokens'].apply(remove_stopwords)

# 5. Stemming (Using Porter Stemmer)
ps = PorterStemmer()
def stem_text(tokens):
    return [ps.stem(word) for word in tokens]

df['tokens_stemmed'] = df['tokens'].apply(stem_text)

# 6. Lemmatization (Using WordNet Lemmatizer)
lemmatizer = WordNetLemmatizer()
def lemmatize_text(tokens):
    return [lemmatizer.lemmatize(word) for word in tokens]

df['tokens_lemmatized'] = df['tokens'].apply(lemmatize_text)

# 7. Join tokens back to string for model input
df['processed_review'] = df['tokens_lemmatized'].apply(lambda x: ' '.join(x))

# 8. Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['processed_review'], df['sentiment'], test_size=0.2, random_state=42)

# 9. Vectorization (BoW or TF-IDF)
# Using TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, ngram_range=(1, 2))
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# 10. Check for NaN values in the transformed data
# Ensure that there are no NaN values in the TF-IDF matrix
if np.any(np.isnan(X_train_tfidf.toarray())):
    print("NaN values found in the training data")
    X_train_tfidf = np.nan_to_num(X_train_tfidf.toarray())

if np.any(np.isnan(X_test_tfidf.toarray())):
    print("NaN values found in the test data")
    X_test_tfidf = np.nan_to_num(X_test_tfidf.toarray())

# 11. Ensure y_train does not contain NaN values
y_train = y_train.dropna()
X_train_tfidf = X_train_tfidf[y_train.index]  # Align X_train_tfidf with y_train's indices

y_test = y_test.dropna()
X_test_tfidf = X_test_tfidf[y_test.index]  # Align X_test_tfidf with y_test's indices

# 12. Logistic Regression Model
logistic_model = LogisticRegression(max_iter=1000)
logistic_model.fit(X_train_tfidf, y_train)

# 13. Make predictions
y_pred = logistic_model.predict(X_test_tfidf)

# 14. Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print('Classification Report:\n', classification_report(y_test, y_pred))
print('Confusion Matrix:\n', confusion_matrix(y_test, y_pred))

# Example usage (prediction on new review)
new_review = "The movie was fantastic and very enjoyable"
processed_new_review = clean_text(new_review)
tokens_new_review = tokenizer.tokenize(processed_new_review)
tokens_no_stopwords = remove_stopwords(tokens_new_review)
tokens_stemmed = stem_text(tokens_no_stopwords)
tokens_lemmatized = lemmatize_text(tokens_stemmed)
processed_new_review = ' '.join(tokens_lemmatized)

# Vectorizing the new review using the trained TF-IDF vectorizer
new_review_vectorized = tfidf_vectorizer.transform([processed_new_review])

# Predict the sentiment of the new review
new_prediction = logistic_model.predict(new_review_vectorized)
print(f"Sentiment of the new review: {new_prediction[0]}")